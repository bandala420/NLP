{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47982669-c6f3-4cd4-a08b-144968a4e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Bandala @ oct 2022\n",
    "from math import log\n",
    "import numpy as np\n",
    "from nltk import download, NLTKWordTokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6555234-1fdb-44de-a92a-0467e8c7b4d0",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction. Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a set of case frames to hold the information contained in a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da05b38-3c69-4787-aa7f-4363d6ea96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bandala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download punkt tokenizer source\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd12d29-85c0-4993-85c3-0d1cd5753b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"No sé con qué armas se peleará la tercera guerra mundial, pero la cuarta se peleará con palos y piedras\"\n",
    "doc2 = \"El fin de la segunda guerra mundial llegó con las bombas atómicas lanzadas en Japón.\"\n",
    "doc3 = \"La casa se está incendiando porque le cayeron bombas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad800fda-4b38-4128-ae23-9777d1d9cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens document 1:  21\n",
      "Tokens document 2:  16\n",
      "Tokens document 3:  10\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "doc1_tok = word_tokenize(doc1)\n",
    "doc2_tok = word_tokenize(doc2)\n",
    "doc3_tok = word_tokenize(doc3)\n",
    "print(\"Tokens document 1: \",len(doc1_tok))\n",
    "print(\"Tokens document 2: \",len(doc2_tok))\n",
    "print(\"Tokens document 3: \",len(doc3_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562ac09a-4de1-43bc-889b-fc2fa10ac5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems document 1:  21\n",
      "Stems document 2:  16\n",
      "Stems document 3:  10\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "doc1_stem = [stemmer.stem(i) for i in doc1_tok]\n",
    "doc2_stem = [stemmer.stem(i) for i in doc2_tok]\n",
    "doc3_stem = [stemmer.stem(i) for i in doc3_tok]\n",
    "print(\"Stems document 1: \",len(doc1_stem))\n",
    "print(\"Stems document 2: \",len(doc2_stem))\n",
    "print(\"Stems document 3: \",len(doc3_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6839105-a2ff-48fa-8d20-77097b9aa61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning\n",
    "docs = [doc1_tok,doc2_tok,doc3_tok]\n",
    "words = [\"guerra\",\"bombas\",\"casa\"]\n",
    "C = len(docs)\n",
    "W = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e2a32e-7ba8-4870-ad99-00714a96b545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0], [1, 1, 0], [0, 1, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of occurrences\n",
    "words_freq = [[docs[j].count(words[i]) for i in range(W)] for j in range(C)]\n",
    "words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "868d2e4d-b1a1-416d-bf6e-7126e037baf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max occurrences\n",
    "max_words_freq = [min(max(words_freq[i]),1) for i in range(C)]\n",
    "max_words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b57495-d899-463e-ba15-667ebcafb6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency matrix\n",
      " doc1 \t doc2 \t doc3 \n",
      "1.0\t1.0\t0.0\n",
      "0.0\t1.0\t1.0\n",
      "0.0\t0.0\t1.0\n"
     ]
    }
   ],
   "source": [
    "# frequency matrix\n",
    "tf = [[docs[j].count(words[i])/max_words_freq[j] for j in range(C)] for i in range(W)]\n",
    "print(\"Frequency matrix\\n doc1 \\t doc2 \\t doc3 \\n\"+'\\n'.join(['\\t'.join([str(cell) for cell in row]) for row in tf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0295586-b2f1-4ab8-bc6c-dc20e7e533fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4054651081081644, 0.4054651081081644, 1.0986122886681098]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse document frequency\n",
    "words_freq = [[docs[i].count(words[j]) for i in range(C)] for j in range(W)]\n",
    "docs_counter = [sum(words_freq[i]) for i in range(W)]\n",
    "idf = [log(C/docs_counter[i]) for i in range(W)]\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35a130d-06ed-4bdb-a9dd-f8d410464ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse document frequency (IDF) of: \n",
      "         guerra: 0.4054651081081644\n",
      "         bombas: 0.4054651081081644\n",
      "           casa: 1.0986122886681098\n"
     ]
    }
   ],
   "source": [
    "print(\"Inverse document frequency (IDF) of: \")\n",
    "for i in range(W):\n",
    "    print(f'{words[i]:>15}: {idf[i]:>10}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e0210f-8991-4fdd-9197-56cdd598effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix\n",
      " doc1 \t\t doc2 \t\t doc3 \n",
      "0.40547\t\t0.40547\t\t0.0\n",
      "0.0\t\t0.40547\t\t0.40547\n",
      "0.0\t\t0.0\t\t1.09861\n"
     ]
    }
   ],
   "source": [
    "# calculate tf-idf matrix\n",
    "tf_idf = [[0 for _ in range(W)] for _ in range(C)] \n",
    "for i in range(W):\n",
    "    for j in range(C):\n",
    "        tf_idf[i][j] = tf[i][j] * idf[i]\n",
    "# print matrix\n",
    "print(\"TF-IDF Matrix\\n doc1 \\t\\t doc2 \\t\\t doc3 \\n\"+'\\n'.join(['\\t\\t'.join([str(round(cell,5)) for cell in row]) for row in tf_idf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca48b4ef-51f4-4630-9656-087c0f03ff52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurrences matrix\n",
      "\t\t doc1 \t\t doc2 \t\t doc3 \n",
      "guerra\t\t1\t\t1\t\t0\n",
      "bombas\t\t0\t\t1\t\t1\n",
      "tercera\t\t1\t\t0\t\t0\n"
     ]
    }
   ],
   "source": [
    "# occurrences matrix\n",
    "words = [\"guerra\",\"bombas\",\"tercera\"]\n",
    "W = len(words)\n",
    "occ_matrix = np.array([[docs[i].count(words[j]) for i in range(C)] for j in range(W)])\n",
    "occ_matrix = np.c_[words,occ_matrix]\n",
    "print(\"Occurrences matrix\\n\\t\\t doc1 \\t\\t doc2 \\t\\t doc3 \\n\"+'\\n'.join(['\\t\\t'.join([str(cell) for cell in row]) for row in occ_matrix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155c59b7-8716-4713-9b3c-4e4cb9db60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean matrix\n",
      "\t\t doc1 \t\t doc2 \t\t doc3 \n",
      "guerra\t\tTrue\t\tTrue\t\tFalse\n",
      "bombas\t\tFalse\t\tTrue\t\tTrue\n",
      "tercera\t\tTrue\t\tFalse\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# boolean matrix for a words set\n",
    "bool_matrix = np.array([[True if docs[i].count(words[j])>0 else False for i in range(C)] for j in range(W)])\n",
    "bool_matrix = np.c_[words,bool_matrix]\n",
    "print(\"Boolean matrix\\n\\t\\t doc1 \\t\\t doc2 \\t\\t doc3 \\n\"+'\\n'.join(['\\t\\t'.join([str(cell) for cell in row]) for row in bool_matrix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42171195-1b35-43b9-885d-232d353693b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with all three words in set:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents with all three words in set:\\n\")\n",
    "for i in range(C):\n",
    "    contains_words = False\n",
    "    for j in range(W):\n",
    "        contains_words = contains_words and bool_matrix[j][i]\n",
    "    if (contains_words):\n",
    "        print(\"Document \",i,\" contains all words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22626c1-0479-48d4-ae01-0afd2b5ae060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
