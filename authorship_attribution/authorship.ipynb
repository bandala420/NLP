{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280c2f93-983d-4353-80b1-422c308b1f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Bandala @ nov 2022\n",
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0561b7-2bb8-46a7-8d21-b6db08b1ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2597bf1c-79b5-40ce-9915-9a7e3fd73069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['elaleph.txt', 'elevangeliosegunmarcos.txt', 'cartaaunasenoritaenparis.txt', 'casatomada.txt']\n"
     ]
    }
   ],
   "source": [
    "#Cargar datos\n",
    "files = [\"elaleph.txt\",\"elevangeliosegunmarcos.txt\",\"cartaaunasenoritaenparis.txt\",\"casatomada.txt\"] # \"articulocientifico.txt\"\n",
    "texts = []\n",
    "for fn in files:\n",
    "    with open(fn) as f:\n",
    "        texts.append(f.read())\n",
    "all_text = ' '.join(texts)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c47169-f57d-4be0-a9e3-255cce47dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    \"\"\"\n",
    "    Use k-means clustering to fit a model\n",
    "    \"\"\"\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=10, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5d4ab68-aa84-43db-bfca-20b04dbe2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalFeatures():\n",
    "    \"\"\"\n",
    "    Compute feature vectors for word and punctuation features\n",
    "    \"\"\"\n",
    "    num_texts = len(texts)\n",
    "    fvs = np.zeros((len(texts), 9), np.float64)\n",
    "    for e, ch_text in enumerate(texts):\n",
    "        contMayus = 0\n",
    "        contMinus = 0\n",
    "        contNumeros = 0\n",
    "        textoNormal = ch_text\n",
    "        textoMinusculas = ch_text.lower()\n",
    "        charTotales = len(textoNormal)\n",
    "        texto = ch_text.lower()\n",
    "        tokens = nltk.word_tokenize(texto)\n",
    "        words = word_tokenizer.tokenize(texto)\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "        for letra in textoNormal:\n",
    "            if letra.isupper():\n",
    "                contMayus = contMayus + 1\n",
    "            else:\n",
    "                contMinus = contMinus + 1\n",
    "            if letra in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]:\n",
    "                contNumeros = contNumeros + 1\n",
    "        # Número promedio de palabras por oración\n",
    "        fvs[e, 0] = words_per_sentence.mean()\n",
    "        # Variación del tamaño de las oraciones\n",
    "        fvs[e, 1] = words_per_sentence.std()\n",
    "        # Diversidad léxica\n",
    "        fvs[e, 2] = len(vocab) / float(len(words))\n",
    "        # Número de comas por oración\n",
    "        fvs[e, 3] = tokens.count(',') / float(len(sentences))\n",
    "        # Número de puntos y comas por oración\n",
    "        fvs[e, 4] = tokens.count(';') / float(len(sentences))\n",
    "        # Número de dos puntos por oración\n",
    "        fvs[e, 5] = tokens.count(':') / float(len(sentences))\n",
    "        # Proporción mayúsculas/total de caracteres\n",
    "        fvs[e, 6] = float(contMayus/charTotales)\n",
    "        # Proporción minúsculas/total de caracteres\n",
    "        fvs[e, 7] = float(contMinus/charTotales)\n",
    "        # Proporción de números respecto a letras\n",
    "        fvs[e, 8] = float(contNumeros/charTotales)\n",
    "\n",
    "    fvs = whiten(fvs)\n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99b2e1c-fb84-4ff9-b544-d9cb7a1d683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "feature_sets = []\n",
    "for element in list(LexicalFeatures()):\n",
    "    feature_sets.append(element)\n",
    "feature_sets = np.array(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c835379d-79bb-4ef0-8fc5-e160f18b370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.51711707   4.06379989  19.84778726   3.70450877   3.16848278\n",
      "    2.99193192   5.6156501  306.66533696   2.6671317 ]\n",
      " [  4.60522708   1.5523925   20.11183412   1.78726301   1.55716034\n",
      "    2.10969558   5.40041235 306.88057471   0.52763767]\n",
      " [  6.90950291   2.65061009  17.74780277   3.31483432   0.55455295\n",
      "    0.69820364   3.23727713 309.04370993   0.        ]\n",
      " [  4.87904394   1.70369342  18.30923726   1.32462143   0.92585536\n",
      "    0.60526095   3.9031455  308.37784156   0.94835694]]\n"
     ]
    }
   ],
   "source": [
    "print(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e2f5ef-3d6f-4595-9859-cc35f7060ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "result = PredictAuthors(feature_sets).labels_\n",
    "print(\"Prediction: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12f802-9e53-4675-bec4-f456b7be3d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Al eliminar el articulo cientifico de los documentos, la clasificación no se realiza de manera correcta, puesto que únicamente un solo documento lo esta clasificando con el autor Jorge Luis Borges. Por tanto, se agregan más características estilométricas para facilitar el trabajo del clasificador. Se agregan 6 características sintácticas y 10 características relacionadas a la bolsa de palabras de todos los documentos, resultando en un total de 25 características estilométricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b334674-d404-4b77-9e1d-cd6bc00316cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures():\n",
    "    \"\"\"\n",
    "    Extract feature vector for part of speech frequencies\n",
    "    \"\"\"\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    chapters_pos = [token_to_pos(ch) for ch in texts]\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[ch.count(pos) for pos in pos_list]\n",
    "                           for ch in chapters_pos]).astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(ch) for ch in chapters_pos])]\n",
    "\n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf1872f9-a95a-4663-9466-8a8b8b6b2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords():\n",
    "    \"\"\"\n",
    "    Compute the bag of words feature vectors, based on the most common words\n",
    "     in the whole book\n",
    "    \"\"\"\n",
    "    # get most common words in the whole book\n",
    "    NUM_TOP_WORDS = 10\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "    fdist = nltk.FreqDist(all_tokens)    \n",
    "    vocab = list(fdist.keys())[:NUM_TOP_WORDS]\n",
    "\n",
    "    # use sklearn to create the bag for words feature vector for each chapter\n",
    "    vectorizer = CountVectorizer(vocabulary=vocab, tokenizer=nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(texts).toarray().astype(np.float64)\n",
    "\n",
    "    # normalise by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "\n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634eec92-bbf0-4e0d-b883-f6525d1dc914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bandala/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get features\n",
    "lexical = LexicalFeatures()\n",
    "syntactic = SyntacticFeatures()\n",
    "word_bag = BagOfWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d247dda-a665-492f-b0a1-2a3581d36bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate arrays\n",
    "feature_sets = np.concatenate((lexical, syntactic, word_bag), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a3c5ba-ed2e-4b8d-ae53-afa0da98772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.51711707e+00 4.06379989e+00 1.98477873e+01 3.70450877e+00\n",
      "  3.16848278e+00 2.99193192e+00 5.61565010e+00 3.06665337e+02\n",
      "  2.66713170e+00 3.43557060e-01 8.04620594e-02 2.33021310e-02\n",
      "  3.06711810e-02 1.06751643e-01 2.54929297e-02 0.00000000e+00\n",
      "  3.35780389e-03 3.35780389e-03 8.15946345e-01 3.35780389e-03\n",
      "  3.82789643e-01 4.33156701e-01 0.00000000e+00 0.00000000e+00\n",
      "  6.71560777e-03]\n",
      " [4.60522708e+00 1.55239250e+00 2.01118341e+01 1.78726301e+00\n",
      "  1.55716034e+00 2.10969558e+00 5.40041235e+00 3.06880575e+02\n",
      "  5.27637671e-01 4.17888563e-01 8.06451613e-02 3.27468231e-02\n",
      "  3.32355816e-02 1.07526882e-01 2.59042033e-02 0.00000000e+00\n",
      "  0.00000000e+00 8.58788400e-03 6.44091300e-01 0.00000000e+00\n",
      "  3.69279012e-01 6.69854952e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [6.90950291e+00 2.65061009e+00 1.77478028e+01 3.31483432e+00\n",
      "  5.54552951e-01 6.98203645e-01 3.23727713e+00 3.09043710e+02\n",
      "  0.00000000e+00 4.04733728e-01 4.52662722e-02 3.07692308e-02\n",
      "  3.99408284e-02 1.19822485e-01 2.69230769e-02 0.00000000e+00\n",
      "  0.00000000e+00 1.53142968e-02 7.86133904e-01 0.00000000e+00\n",
      "  3.82857421e-01 4.84952733e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.87904394e+00 1.70369342e+00 1.83092373e+01 1.32462143e+00\n",
      "  9.25855359e-01 6.05260952e-01 3.90314550e+00 3.08377842e+02\n",
      "  9.48356939e-01 4.06078724e-01 4.83308421e-02 3.53761834e-02\n",
      "  4.63378176e-02 1.08121574e-01 2.93971101e-02 0.00000000e+00\n",
      "  0.00000000e+00 9.30282872e-03 8.18648928e-01 0.00000000e+00\n",
      "  4.37232950e-01 3.72113149e-01 0.00000000e+00 0.00000000e+00\n",
      "  9.30282872e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(feature_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7faa27ac-7c77-4489-801c-bebc0362f155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "result = PredictAuthors(feature_sets).labels_\n",
    "print(\"Prediction: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac35e8e-1106-4929-ab0c-b241ab5797c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
