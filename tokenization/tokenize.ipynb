{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47982669-c6f3-4cd4-a08b-144968a4e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daniel Bandala @ sep 2022\n",
    "from nltk import download, NLTKWordTokenizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,TweetTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a0bd1-6d63-4439-9b7b-2f55b143d500",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Natural Language Processing (NLP) enables machine learning algorithms to organize and understand human language. NLP enables machines to not only gather text and speech but also identify the core meaning it should respond to. Human language is complex, and constantly evolving, which means natural language processing has quite the challenge. Tokenization is one of the many pieces of the puzzle in how NLP works. In this sense, tokenization is a simple process that takes raw data and converts it into a useful data string. While tokenization is well known for its use in cybersecurity and in the creation of NFTs, tokenization is also an important part of the NLP process. Tokenization is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. In general, tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types â€“ word, character, and subword (n-gram characters) tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da05b38-3c69-4787-aa7f-4363d6ea96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bandala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download punkt tokenizer source\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd12d29-85c0-4993-85c3-0d1cd5753b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"God is Great! I won a lottery.\"\n",
    "# word tokenization\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4358f65-5e06-4562-bbd1-746de24ff4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336757c6-7603-4dcb-9220-cfc939dc09d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "print(word_tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4d91c15-cdb8-4826-804f-79c7214111eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'We', 'beat', 'some', 'pretty', 'good', 'teams', 'to', 'get', 'here', ',', \"''\", 'Slocum', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "s2 = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\"\n",
    "print(word_tokenize(s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdc035-8868-40c1-b504-270f08b4b59f",
   "metadata": {},
   "source": [
    "### Gathering the spans of the tokenized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656b62d2-fd33-49d8-ac3f-0ddad70a8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''Good muffins cost $3.88\\nin New (York).  Please (buy) me\\ntwo of them.\\n(Thanks).'''\n",
    "expected = [(0, 4), (5, 12), (13, 17), (18, 19), (19, 23),\n",
    "            (24, 26), (27, 30), (31, 32), (32, 36), (36, 37), (37, 38),\n",
    "            (40, 46), (47, 48), (48, 51), (51, 52), (53, 55), (56, 59),\n",
    "            (60, 62), (63, 68), (69, 70), (70, 76), (76, 77), (77, 78)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc96417-b2ed-48fe-ad38-b3ebaff2688e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(NLTKWordTokenizer().span_tokenize(s)) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ea417a-82ae-4f9e-bf41-3d2b6caa3780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = ['Good', 'muffins', 'cost', '$', '3.88', 'in',\n",
    "            'New', '(', 'York', ')', '.', 'Please', '(', 'buy', ')',\n",
    "            'me', 'two', 'of', 'them.', '(', 'Thanks', ')', '.']\n",
    "[s[start:end] for start, end in NLTKWordTokenizer().span_tokenize(s)] == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf64fd-ff86-486b-a29c-0286899f17b4",
   "metadata": {},
   "source": [
    "### Testing improvement made to the TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdcebb3e-0df9-49c6-b528-7f5fe38528b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx1 = '\\xabNow that I can do.\\xbb'\n",
    "expected = ['\\xab', 'Now', 'that', 'I', 'can', 'do', '.', '\\xbb']\n",
    "word_tokenize(sx1) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3b9594-e862-4e6d-9507-3feec8f39d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx2 = 'The unicode 201C and 201D \\u201cLEFT(RIGHT) DOUBLE QUOTATION MARK\\u201d is also OPEN_PUNCT and CLOSE_PUNCT.'\n",
    "expected = ['The', 'unicode', '201C', 'and', '201D', '\\u201c', 'LEFT', '(', 'RIGHT', ')', 'DOUBLE', 'QUOTATION', 'MARK', '\\u201d', 'is', 'also', 'OPEN_PUNCT', 'and', 'CLOSE_PUNCT', '.']\n",
    "word_tokenize(sx2) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56da4a7e-f5d0-4752-84d4-fb53e0b0db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer = TreebankWordDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b1e5e1-7149-428a-ad3f-b5d2b754c299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.\"\n",
    "detokenizer.detokenize(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56669497-31c3-4972-9808-84506fbf5483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"We beat some pretty good teams to get here,\" Slocum said.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\\\"We beat some pretty good teams to get here,\\\" Slocum said.\"\n",
    "detokenizer.detokenize(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dccabf47-b011-439e-a4d7-c05563aa18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well, we couldn\\'t have this predictable, cliche-ridden, \"Touched by an Angel\" (a show creator John Masius worked on) wanna-be if she didn\\'t.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Well, we couldn't have this predictable, cliche-ridden, \\\"Touched by an Angel\\\" (a show creator John Masius worked on) wanna-be if she didn't.\"\n",
    "detokenizer.detokenize(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f27c9b-2c6a-400e-9d9c-8738d91f0a70",
   "metadata": {},
   "source": [
    "### TweetTokenizer\n",
    "TweetTokenizer is a tokenizer specifically designed for micro-blogging tokenization tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b926a1f-cc8b-4d27-a0a4-507d27c4e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7a0c43-751c-42e5-bc45-6975c7be1e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "print(tknzr.tokenize(s0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5771ea9b-611e-4998-95dc-c80d7416bdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@Joyster2012', '@CathStaincliffe', 'Good', 'for', 'you', ',', 'girl', '!', '!', 'Best', 'wishes', ':-)']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"@Joyster2012 @CathStaincliffe Good for you, girl!! Best wishes :-)\"\n",
    "print(tknzr.tokenize(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee6a30e-1f5a-4980-a2c7-b936ab1e1489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3Points', 'for', '#DreamTeam', 'Gooo', 'BAILEY', '!', ':)', '#PBB737Gold', '@PBBabscbn']\n"
     ]
    }
   ],
   "source": [
    "s2 = \"3Points for #DreamTeam Gooo BAILEY! :) #PBB737Gold @PBBabscbn\"\n",
    "print(tknzr.tokenize(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93705a92-5c2b-4426-b2a1-f021479c3066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
